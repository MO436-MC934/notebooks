{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38a2711",
   "metadata": {},
   "source": [
    "# 5.3. JAX Internals and Primitives\n",
    "\n",
    "> **Author**: Gustavo Leite / **Date**: March 2022.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b27b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRARY IMPORTS\n",
    "# ===============================================================================================\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "# LOCAL IMPORTS\n",
    "# ===============================================================================================\n",
    "\n",
    "# Utilitary code used in this notebook\n",
    "# Provides functions for tracing and logging\n",
    "# Source: ./support/tracing.py\n",
    "import util.tracing as tracing\n",
    "\n",
    "# EXTERNAL IMPORTS\n",
    "# ===============================================================================================\n",
    "\n",
    "import numpy as np      # Standard Numpy\n",
    "import jax.numpy as jnp # JAX Numpy namespace that mimics standard Numpy\n",
    "\n",
    "from jax import (\n",
    "    jit,         # Transformation for just-in-time compiling\n",
    "    grad,        # Transformation for computing the gradient (i.e. derivative)\n",
    "    jvp,         # Transformation for jacobian-vector product (forward autodifferentiation)\n",
    "    vjp,         # Transformation for vector-jacobian product (backward autodifferentiation)\n",
    "    vmap,        # Transformation for vectorization\n",
    "    pmap,        # Transformation for parallelization\n",
    "    make_jaxpr,  # Transformation for dumping the corresponding JAXPR\n",
    "    lax,         # Namespace where default primitives are defined\n",
    ")\n",
    "\n",
    "from jax.core import (\n",
    "    Primitive,   # Object denoting a primitive operation\n",
    "    ShapedArray, # Object denoting an abstract array with a shape and a type\n",
    ")\n",
    "\n",
    "from jax._src.lib import (\n",
    "    xla_client,  # Namespace that allows us to interact with the XLA Python API\n",
    ")\n",
    "\n",
    "from jax.interpreters import (\n",
    "    xla,         # Register primitives for compilation with XLA\n",
    "    ad,          # Register primitives for auto-differentiation\n",
    "    batching,    # Register primitives for batching\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b01691",
   "metadata": {},
   "source": [
    "## PART 0: Introduction\n",
    "\n",
    "JAX offers a series of composable function transformations like JIT compilation, auto-differentiation, auto-vectorization, and more. In order to carry out these transformations, JAX needs to know about the \"structure\" of the function being transformed. For instance, take the case of autodiff: JAX needs to know, somehow, which mathematical operations are being applied to each input parameter in order to be able to compute its derivative analitically. This evokes an idea of an abstract representation of the function, like a *computation graph*, that relates inputs and operations to the outputs of the function. The operations in this computation graph are called **Primitives** and they will be the main object of study in this notebook.\n",
    "\n",
    "The next logical question is: how do we obtain such computation graph? If JAX was part of the Python interpreter, it could simply get a reference of the Abstract Syntax Tree (AST) of the function and make transformations based on it. However, JAX is distributed as a library therefore it needs another mechanism of obtaining the computation graph: **tracing**. The tracing mechanism consists of calling the function to be transformed with special objects in place of the inputs. These special objects overload the mathematical operations in Python and a able to \"record\" everything that is done to them. This record of operatins is precisely what we a looking for: a computation graph. In JAX terms, this computation graph is calleda JAXPR, short for *JAX Expression*. The special objects used in place of the real parameters are called Tracers. We will talk more about them later.\n",
    "\n",
    "The next cell shows an extremely simplified example of tracing a function by overloading binary operators. The class `AbstractVar` represents an abstract variable: it has a name but its not have a value; the class `Expression` represents a recursive structure of computations performed on abstract variables; and the class `Tracer` manages to create the structure of expressions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c3284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invocation with real inputs:\n",
      "==> 24.0\n",
      "\n",
      "Invocation with Tracer inputs:\n",
      "==> Expression(op='add', lhs=Expression(op='pow', lhs=AbstractVar(name='x'), rhs=2.0), rhs=Expression(op='mul', lhs=AbstractVar(name='x'), rhs=AbstractVar(name='y')))\n"
     ]
    }
   ],
   "source": [
    "# In this cell we recreate the JAX tracing mechanism in a extremely simplified way.\n",
    "\n",
    "class AbstractVar(NamedTuple):\n",
    "    \"\"\"Class that represents an abstract variable.\"\"\"\n",
    "    name: str\n",
    "    \n",
    "\n",
    "class Expression(NamedTuple):\n",
    "    \"\"\"Class that represents a binary operation.\"\"\"\n",
    "    op: str\n",
    "    lhs: object\n",
    "    rhs: object\n",
    "        \n",
    "        \n",
    "class Tracer(object):\n",
    "    \"\"\"Tracer object that `records` operations.\"\"\"\n",
    "    value: object\n",
    "        \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return repr(self.value)\n",
    "        \n",
    "    # Overload the '+' (addition) operation\n",
    "    def __add__(self, rhs):\n",
    "        return Tracer(Expression(op=\"add\", lhs=self, rhs=rhs))\n",
    "    \n",
    "    # Overload the '*' (multiplication) operation\n",
    "    def __mul__(self, rhs):\n",
    "        return Tracer(Expression(op=\"mul\", lhs=self, rhs=rhs))\n",
    "    \n",
    "    # Overload the '**' (power) operation\n",
    "    def __pow__(self, rhs):\n",
    "        return Tracer(Expression(op=\"pow\", lhs=self, rhs=rhs))\n",
    "    \n",
    "    # Ideally, we would overload ALL of python operators, this is exactly what JAX does.\n",
    "    # We overload only a few of them here to keep the example small.\n",
    "    ...\n",
    "\n",
    "    \n",
    "def my_function(x, y):\n",
    "    \"\"\"This is the function we would like to trace.\n",
    "    \n",
    "    Note how it is pure: we comput the output from the inputs only.\"\"\"\n",
    "    return x ** 2. + x * y\n",
    "\n",
    "\n",
    "# Let's test with real inputs\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Invocation with real inputs:\\n==> \", end=\"\")\n",
    "result = my_function(2., 10.)  # Call function with regular floats\n",
    "print(result)\n",
    "\n",
    "# Now test with Tracer inputs\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nInvocation with Tracer inputs:\\n==> \", end=\"\")\n",
    "X = Tracer(AbstractVar(\"x\"))   # Create a tracer to represent input `x`\n",
    "Y = Tracer(AbstractVar(\"y\"))   # Create a tracer to represent input `y`\n",
    "result = my_function(X, Y)     # Call our function with tracers instead of numbers\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735ea8f",
   "metadata": {},
   "source": [
    "Take a moment to understand what this example is doing. On the first invocation of `my_function` we passed floating points as parameters and got another floating point as output, just as we expect. On the second invocation, however, we passed `Tracer` objects and it returned another tracer that encapsulates the recursive structure of operations. Isn't is interesting that we can discover the structure of a function without really computing it?\n",
    "\n",
    "The recursive structure returned in the second invocations can be understood as a directed acyclic graph (DAG).\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/expression_tree.png\" alt=\"Expression tree.\" width=\"60%\" />\n",
    "</center>\n",
    "\n",
    "> **Note**: Curious readers who would like to know more about the tracing mechanism are referred to this [notebook](https://jax.readthedocs.io/en/latest/autodidax.html) in the JAX Docs.\n",
    "\n",
    "One limitation of this tracing approach, is that JAX can only trace mathematical operations like addition, multiplication, division, etc. It cannot, for instance, detect when a variable is printed to standard output or to a file. In this sense, JAX expects functions to be *pure*. In other words, the function being transformed should produce its output based solely on its inputs and not depend on any external implicit value (*e.g.* global variables, files, etc). Another way to put it is to say that functions must be *free of side-effects*.\n",
    "\n",
    "When the computation graph is available, JAX can traverse it and do whatever it needs to do: compile operations if doing JIT, differentiating expressions if doing autodiff, you get the idea. Based on this scenario, we will explore how can we expand the default operations offered by JAX and create new primitives that are compatible with existing transformations.\n",
    "\n",
    "In the PART 1 of this document we will discuss..\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. JAX Primitives and JIT\n",
    "2. JAX Primitives and Autodiff\n",
    "3. JAX Primitives and Batching\n",
    "4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba731049",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## PART 1: JAX Primitives and JIT\n",
    "\n",
    "\n",
    "### Preparing the Ground\n",
    "\n",
    "Say that we want to implement a function `square_add(a, b)` that returns $a^2 + b$ and we want to use the `multiply_add(x, y, z): return x * y + z` function to implement `square_add`. Using JAX Numpy, we could write these functions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7f0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_numpy(x, y, z):\n",
    "    \"\"\"Multiply `x` by `y` and add `z`.\"\"\"\n",
    "    return jnp.add(jnp.multiply(x, y), z)\n",
    "#          ~~~     ~~~\n",
    "#           `-------`--> We are using JAX Numpy functions but\n",
    "#                        they do exactly the same thing as the\n",
    "#                        standard numpy functions `np.add` and\n",
    "#                        `np.multiply`.\n",
    "\n",
    "@tracing.log_calls\n",
    "def square_add_numpy(a, b):\n",
    "    \"\"\"Square `a` and add `b`\"\"\"\n",
    "    return multiply_add_numpy(a, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882dd7e",
   "metadata": {},
   "source": [
    "> **Note**: The decorator `@tracing.log_calls` will log the invocations of the function being decorated to stdout. Nested invocations are shown with indentation for improved readability, you will see what that means in the next cell.\n",
    "\n",
    "Let us now call our new `square_add_numpy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094373e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_numpy(2.0, 10.0)\n",
      "|   | CALL multiply_add_numpy(2.0, 2.0, 10.0)\n",
      "|   | RET  multiply_add_numpy = 14.0\n",
      "| RET  square_add_numpy = 14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(14., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_add_numpy(2., 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fe6eb",
   "metadata": {},
   "source": [
    "Immediately, some things come to our attention:\n",
    "\n",
    "1. The nested calling and returning from functions is dumped to standard output;\n",
    "2. As expected, `square_add_numpy` receives two floats as input (2 and 10) and returns another float (14);\n",
    "3. The result of this computation is an object of type `DeviceArray`.\n",
    "\n",
    "Apart from the formatted call stack, this is exactly what we expect from evaluating this function. Now let us try using the `grad` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3104f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_numpy(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   | CALL multiply_add_numpy(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   | RET  multiply_add_numpy = Tracer<ConcreteArray(14.0, dtype=float32, weak_type=True)>\n",
      "| RET  square_add_numpy = Tracer<ConcreteArray(14.0, dtype=float32, weak_type=True)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(4., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(square_add_numpy)(2., 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a83f09",
   "metadata": {},
   "source": [
    "New observations can be made:\n",
    "\n",
    "1. There is our call stack again;\n",
    "2. The floating point parameters we passed were wrapped in a `Tracer<ConcreteArray(...)>`; (note something familiar?)\n",
    "3. Returned number of 4.0 which is exactly the derivative of `square_add(a,b)` with respect to `a`.\n",
    "\n",
    "Remember the example of the introduction? Well, the `Tracer` and `ConcreteArray` objects are some of the mechanisms that allow the tracing to occurr inside JAX. The `Tracer` class has the exact same purpose as our initial example and the `ConcreteArray` is called and abstract value, and it serves as a generalization of the `Variable` class from the example in the introduction. There are many types of abstract values in JAX and each of those types correspond to how much we know about a variable of a value:\n",
    "\n",
    "- `ConcreteArray`s mean that we know the type, the shape and the CONTENTS of the variable.\n",
    "- `ShapedArray`s mean that we know only the type and shape, but not the contents.\n",
    "\n",
    "Similarly for `Tracer`s, there are a couple of them in JAX, each one specialized for a particular transformation. The class diagram below gathers a non-exhaustive list of the tracers and abstract values found in JAX source code. Classes highlighted in orange work as abstract base classes.\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/avals_and_tracers.png\" alt=\"Abstract values and tracers in JAX\" width=\"50%\" />\n",
    "</center>\n",
    "\n",
    "<hr />\n",
    "\n",
    "Up until this point we only used JAX built-in features to write our functions. Now we are going to recreate the `multiply_add` operation from the ground up, *i.e.* without using `jnp.*` functions. For that need to create a new `Primitive`!\n",
    "\n",
    "### Creating a new Primitive\n",
    "\n",
    "A primitive is simply an object with a name which represents an operation in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a819dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our new primitive\n",
    "multiply_add_p = Primitive(\"multiply_add\")\n",
    "\n",
    "@tracing.log_calls\n",
    "def multiply_add_prim(x, y, z):\n",
    "    \"\"\"The JAX-traceable way to use the JAX primitive.\n",
    "    \n",
    "    This is the function that will be exposed to the user.\"\"\"\n",
    "    return multiply_add_p.bind(x, y, z)\n",
    "#                         ~~~~\n",
    "#                           `-> This `bind` is the important part!\n",
    "\n",
    "@tracing.log_calls\n",
    "def square_add_prim(a, b):\n",
    "    \"\"\"The JAX-traceable way to use the JAX primitive.\n",
    "    \n",
    "    This is another function that will be exposed to the user.\"\"\"\n",
    "    return multiply_add_p.bind(a, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e622c",
   "metadata": {},
   "source": [
    "In the above cell we `bind` the inputs to the `Primitive`. Binding means that we take the concrete inputs from the `multiply_add_prim` function and wrap them with `Tracer`s and `AbstractValue`s. For reasons that will become clear in a moment, binding primitives is the entry point for all transformations and the actual type of the tracer and abstract value determines which kind of information will be extracted during tracing of this primitive.\n",
    "\n",
    "With out primitive instantiated, let's try to call our new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f656978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(2.0, 10.0)\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/367851666.py\", line 2, in <module>\n",
      "    square_add_prim(2., 10.)\n",
      "  File \"/scratch/research/notebooks/gustavo.leite/util/tracing.py\", line 53, in fn_wrapper\n",
      "    res = fn(*args)\n",
      "  File \"/tmp/ipykernel_6900/126395883.py\", line 18, in square_add_prim\n",
      "    return multiply_add_p.bind(a, a, b)\n",
      "NotImplementedError: Evaluation rule for 'multiply_add' not implemented\n"
     ]
    }
   ],
   "source": [
    "with tracing.ExpectNotImplemented():\n",
    "    square_add_prim(2., 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b1d87",
   "metadata": {},
   "source": [
    "We got an error! Pay attention to the error message:\n",
    "\n",
    "> **NotImplementedError**: Evaluation rule for 'multiply_add' not implemented\n",
    "\n",
    "This error was expected. Even though we created a new primitive and gave it a name, we never told JAX what this primitive actually computes! We should define an *implementation* for this primitive, or as JAX put it, an *evaluation rule*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc5e5c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.multiply_add_impl(x, y, z)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_impl(x, y, z):\n",
    "  \"\"\"Concrete implementation of the primitive.\n",
    "\n",
    "  This function does NOT need to be JAX traceable.\n",
    "  \"\"\"\n",
    "  # Note that we can use the original numpy, which is not JAX traceable\n",
    "  return np.add(np.multiply(x, y), z)\n",
    "#           ~~~    ~~~~~~~~\n",
    "#            `--------`--> Same computation as before, but using standard Numpy.\n",
    "\n",
    "\n",
    "# Register the implementation of `multiply_add_p` primitive.\n",
    "multiply_add_p.def_impl(multiply_add_impl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0148bb2",
   "metadata": {},
   "source": [
    "Now we can call our new function that binds on our primitive! Please be mindful that we are not doing JIT yet. This is simply delegating the computation to standard Numpy as is done inside the `multiply_add_impl` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5808171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(2.0, 10.0)\n",
      "|   | CALL multiply_add_impl(2.0, 2.0, 10.0)\n",
      "|   | RET  multiply_add_impl = 14.0\n",
      "| RET  square_add_prim = 14.0\n"
     ]
    }
   ],
   "source": [
    "assert square_add_prim(2., 10.) == 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30facd",
   "metadata": {},
   "source": [
    "Be mindful that this evaluation rule is executed on the CPU no matter what backend you are using. In order to run code on other devices we need to generate code for them. That will come later! ðŸ˜‰\n",
    "\n",
    "Let's review our intuition at this point. Our functions are called in this order (arrows go from caller to callee).\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/primitive_magic.png\" alt=\"Primitive magic\" width=\"80%\" />\n",
    "</center>\n",
    "\n",
    "What magic ðŸª„âœ¨ is going on here? The magic is called `Trace`! (Not to be confused with `Tracer` with an **r**, this is confusing, I know ðŸ¤·). A Trace is an object in the JAX core that process a primitive. There are many types of Traces and each of those roughly correspond to a specific transformation.\n",
    "\n",
    "- `EvalTrace` is responsible for simply calling the primitive implementation;\n",
    "- `DynamicJaxprTrace` is responsible for doing abstract evaluation (more on that later);\n",
    "- `JVPTrace` is responsible for taking the gradient;\n",
    "- `BatchTrace` is responsible for vectorization and parallelization;\n",
    "- etc.\n",
    "\n",
    "Because we are not doing any transformations (yet), JAX defaults to the `EvalTrace` that simply calls the primitive implementation for us. Our intuition is now updated.\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/primitive_eval.png\" alt=\"Primitive magic\" width=\"80%\" />\n",
    "</center>\n",
    "\n",
    "In fact, JAX keeps a stack of traces. When we bind a primitive, it simply takes whatever trace is active (on top of the stack) and calls the `process_primitive` on that trace passing the primitive being binded. The following diagram gives a high-level overview of this process. While processing a primitive, other primitives may be found and the process goes on recursively, until we have traced the entire function.\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/primitive_full.png\" alt=\"Primitive magic\" width=\"80%\" />\n",
    "</center>\n",
    "\n",
    "Having understood that, we can update the class diagram of the JAX core classes. The `Trace`s are the ones responsible for instantiating `Tracer`s of the correct type.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/jax_core_simple.png\" alt=\"Primitive magic\" width=\"100%\" />\n",
    "</center>\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Just-in-Time Compiling our Primitive\n",
    "\n",
    "So far so good, we created a new primitive and gave it an evaluation rule. What if we decide to accelerate our code by JIT compiling our function? Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf99901c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>)\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/2077843376.py\", line 2, in <module>\n",
      "    jit(square_add_prim)(2., 10.)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 165, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 430, in cache_miss\n",
      "    out_flat = xla.xla_call(\n",
      "NotImplementedError: Abstract evaluation for 'multiply_add' not implemented\n"
     ]
    }
   ],
   "source": [
    "with tracing.ExpectNotImplemented():\n",
    "    jit(square_add_prim)(2., 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b17b",
   "metadata": {},
   "source": [
    "We get an error... JAX complains that we have not defined an abstract evaluation rule. Such abstract evaluation rule is used to compute the shape and data type of the output given the shape and data type of the inputs. Remember that JAX compiles your function with the specific input shape in mind, if you try to call the function with inputs of another size, it will trigger a recompilation.\n",
    "\n",
    "We define the **abstract evaluation** rule in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da24e7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.multiply_add_abstract_eval(xs, ys, zs)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_abstract_eval(xs, ys, zs):\n",
    "    \"\"\"Abstractly evaluate our primitive based on the input shapes.\"\"\"\n",
    "    # Assert that all input parameters have the same shape\n",
    "    assert xs.shape == ys.shape\n",
    "    assert xs.shape == zs.shape\n",
    "    # Inform that the output has the same shape as the inputs\n",
    "    return ShapedArray(xs.shape, xs.dtype)\n",
    "\n",
    "\n",
    "# Register the abstract implementation of `multiply_add_p` primitive.\n",
    "multiply_add_p.def_abstract_eval(multiply_add_abstract_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec606d",
   "metadata": {},
   "source": [
    "With that out of the way, let's try calling our primitive one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83a39f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>)\n",
      "|   | CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
      "|   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "| RET  square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)>\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 191, in _run_module_as_main\n",
      "    msg = \"%s: %s\" % (sys.executable, exc)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 75, in _run_code\n",
      "    fname = mod_spec.origin\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 12, in <module>\n",
      "    if sys.path[0] == '':\n",
      "jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: MLIR translation rule for primitive 'multiply_add' not found for platform gpu\n",
      "\n",
      "The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n",
      "\n",
      "--------------------\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/2077843376.py\", line 2, in <module>\n",
      "    jit(square_add_prim)(2., 10.)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 165, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 430, in cache_miss\n",
      "    out_flat = xla.xla_call(\n",
      "NotImplementedError: MLIR translation rule for primitive 'multiply_add' not found for platform gpu\n"
     ]
    }
   ],
   "source": [
    "with tracing.ExpectNotImplemented():\n",
    "    jit(square_add_prim)(2., 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f2272",
   "metadata": {},
   "source": [
    "Another error! Now JAX is upset because it doesn't known how to translate our function to an IR XLA understands so that the function can finally be compiled to native executable binary. This is aptly called a translation rule. We use the XLA API o create and *add* and *mul* instruction. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias for convenience\n",
    "xops = xla_client.ops\n",
    "\n",
    "@tracing.log_calls\n",
    "def multiply_add_xla_translation(ctx, avals_in, avals_out, xc, yc, zc):\n",
    "    \"\"\"Translate our function to XLA's intermediate-representation.\"\"\"\n",
    "    return [xops.Add(xops.Mul(xc, yc), zc)]\n",
    "#           ~~~~~~~~ ~~~~~~~~\n",
    "#               `--------`--> We are creating the IR for our function here!\n",
    "\n",
    "\n",
    "# Associate the new translation rule with our primitive\n",
    "# We use the same translation function for both CPU and GPU, but we need to inform them separately.\n",
    "xla.register_translation(multiply_add_p, multiply_add_xla_translation, platform='cpu')\n",
    "xla.register_translation(multiply_add_p, multiply_add_xla_translation, platform='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b1710",
   "metadata": {},
   "source": [
    "Finally, JIT should be working now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3b1160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>)\n",
      "|   | CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
      "|   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "| RET  square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)>\n",
      "| CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
      "| RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "| CALL multiply_add_xla_translation(TranslationContext(builder=<jaxlib.xla_extension.XlaBuilder object at 0x7ff6e416b370>, platform='gpu', axis_env=AxisEnv(nreps=1, names=(), sizes=()), name_stack=''), [ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], [ShapedArray(float32[])], <XlaOp at 0x7ff6e4171870>, <XlaOp at 0x7ff6e73dd0b0>, <XlaOp at 0x7ff7106272f0>)\n",
      "| RET  multiply_add_xla_translation = [<jaxlib.xla_extension.XlaOp object at 0x7ff74e74e930>]\n"
     ]
    }
   ],
   "source": [
    "assert jit(square_add_prim)(2., 10.) == 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dde2cd",
   "metadata": {},
   "source": [
    "As you can see, invoking our primitive causes it first to be abstract evaluated, then translated using the rule we defined and finally executed on the device. We could also optionally inform JAX that some parameters to this function are static using the `static_argnums=1`. We are saying that the parameter with index 1 of the function `square_add_prim` is static and should not be traced. Compare the first line of the trace above with the first line of the trace below. In the former, `square_add_prim` is invoked with two tracers while on the latter the second parameter is a literal value `10.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d41f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, 10.0)\n",
      "|   | CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
      "|   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "| RET  square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)>\n",
      "| CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
      "| RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "| CALL multiply_add_xla_translation(TranslationContext(builder=<jaxlib.xla_extension.XlaBuilder object at 0x7ff6e4173630>, platform='gpu', axis_env=AxisEnv(nreps=1, names=(), sizes=()), name_stack=''), [ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], [ShapedArray(float32[])], <XlaOp at 0x7ff6e4173c70>, <XlaOp at 0x7ff6e41737b0>, <XlaOp at 0x7ff6e4173970>)\n",
      "| RET  multiply_add_xla_translation = [<jaxlib.xla_extension.XlaOp object at 0x7ff6e416bb70>]\n"
     ]
    }
   ],
   "source": [
    "assert jit(square_add_prim, static_argnums=1)(2., 10.) == 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412f513",
   "metadata": {},
   "source": [
    "We are done! At least with the JIT transformation. Let us compare the two implementations: (a) the one that uses regular `jnp.*` functions; and the (b) the second that implements a new primitive.\n",
    "\n",
    "Here is the JAXPR and MLIR of (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a64ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[] b:f32[]. let c:f32[] = mul a a; d:f32[] = add c b in (d,) }\n",
      "================================================================================\n",
      "module @jit_square_add_numpy.11 {\n",
      "  func public @main(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {\n",
      "    %0 = mhlo.multiply %arg0, %arg0 : tensor<f32>\n",
      "    %1 = mhlo.add %0, %arg1 : tensor<f32>\n",
      "    return %1 : tensor<f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tracing.SuppressCallLog():\n",
    "    print(make_jaxpr(square_add_numpy)(2., 10.))\n",
    "    print(\"=\" * 80)\n",
    "    print(jit(square_add_numpy).lower(2., 10.).compiler_ir('mhlo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef5fe8",
   "metadata": {},
   "source": [
    "And here is the JAXPR and MLIR of (b). Note how instead of `add` and `mul` operations we have a single `multiply_add` in the JAXPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6020fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[] b:f32[]. let c:f32[] = multiply_add a a b in (c,) }\n",
      "================================================================================\n",
      "module @jit_square_add_prim.12 {\n",
      "  func public @main(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {\n",
      "    %0 = call @multiply_add(%arg0, %arg0, %arg1) : (tensor<f32>, tensor<f32>, tensor<f32>) -> tensor<f32>\n",
      "    return %0 : tensor<f32>\n",
      "  }\n",
      "  func private @multiply_add(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {\n",
      "    %0 = call @xla_fallback_multiply_add(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<f32>, tensor<f32>) -> tensor<f32>\n",
      "    return %0 : tensor<f32>\n",
      "  }\n",
      "  func private @xla_fallback_multiply_add(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<f32> {\n",
      "    %0 = mhlo.constant dense<false> : tensor<i1>\n",
      "    %1 = mhlo.multiply %arg0, %arg1 : tensor<f32>\n",
      "    %2 = mhlo.add %1, %arg2 : tensor<f32>\n",
      "    return %2 : tensor<f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tracing.SuppressCallLog():\n",
    "    print(make_jaxpr(square_add_prim)(2., 10.))\n",
    "    print(\"=\" * 80)\n",
    "    print(jit(square_add_prim).lower(2., 10.).compiler_ir('mhlo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf47dc",
   "metadata": {},
   "source": [
    "### What have we done so far...\n",
    "\n",
    "The following table summarizes what we needed to do in order to implement a new primitive and make it work with the `jit` transformation.\n",
    "\n",
    "| Step | Description | API |\n",
    "|:----:|:------------|:----|\n",
    "| 1 | Create new primitive object | `Primitive(NAME)` |\n",
    "| 2 | Define concrete evaluation rule | `PRIMITIVE.def_impl(IMPL_FN)` |\n",
    "| 3 | Define abstract evaluation rule | `PRIMITIVE.def_abstract_eval(ABS_EVAL_FN)` |\n",
    "| 4 | Define translation rule | `xla.register_translation(PRIMITIVE, TRANSLATION_FN)` |\n",
    "\n",
    "In the next section we will enable our primitive to be used with `grad`.\n",
    "\n",
    "<hr />\n",
    "\n",
    "## PART 2: JAX Primitives and Autodiff\n",
    "\n",
    "> **Note**: This section is incomplete. You can stop reading here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb31efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(10.0, dtype=float32, weak_type=True)>)\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/635092508.py\", line 2, in <module>\n",
      "    jvp(square_add_prim, (2., 10.), (1., 1.))\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 2280, in jvp\n",
      "    return _jvp(lu.wrap_init(fun), primals, tangents, has_aux=has_aux)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 2309, in _jvp\n",
      "    out_primals, out_tangents = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)\n",
      "NotImplementedError: Differentiation rule for 'multiply_add' not implemented\n"
     ]
    }
   ],
   "source": [
    "with tracing.ExpectNotImplemented():\n",
    "    jvp(square_add_prim, (2., 10.), (1., 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f1771",
   "metadata": {},
   "source": [
    "We have this function:\n",
    "\n",
    "$$ f(x, y, z) = x \\cdot y + z $$\n",
    "\n",
    "The first partial derivatives are:\n",
    "\n",
    "$$\n",
    "    \\nabla f = (y, x, 1)\n",
    "$$\n",
    "\n",
    "And we are computing its JVP as:\n",
    "\n",
    "$$\n",
    "    \\vec{t} \\cdot \\nabla f = \\nabla_\\vec{t} f = x_t y + y_t x + z_t\n",
    "$$\n",
    "\n",
    "What is happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ac01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_value_and_jvp(arg_values, arg_tangents):\n",
    "    \"\"\"Evaluates the primal output and the tangents (Jacobian-vector product).\n",
    "\n",
    "    Given values of the arguments and perturbation of the arguments (tangents), \n",
    "    compute the output of the primitive and the perturbation of the output.\n",
    "\n",
    "    This method must be JAX-traceable. JAX may invoke it with abstract values \n",
    "    for the arguments and tangents.\n",
    "    \"\"\"\n",
    "    x,  y,  z  = arg_values\n",
    "    xt, yt, zt = arg_tangents\n",
    "    \n",
    "    tracing.log(\">>> Primal evaluation:\")\n",
    "    primal_out = multiply_add_prim(x, y, z)\n",
    "    \n",
    "    def make_zero(tan):\n",
    "        return lax.zeros_like_array(x) if type(tan) is ad.Zero else tan\n",
    "    \n",
    "    tracing.log(\">>> Tangent evaluation:\")\n",
    "    output_tangent = multiply_add_prim(make_zero(xt), y, multiply_add_prim(x, make_zero(yt), make_zero(zt)))\n",
    "    \n",
    "    return primal_out, output_tangent\n",
    "\n",
    "# Register JVP rule for out `multiply_add_p` primitive\n",
    "ad.primitive_jvps[multiply_add_p] = multiply_add_value_and_jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb3fbfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(10.0, dtype=float32, weak_type=True)>)\n",
      "|   | CALL multiply_add_value_and_jvp((2.0, 2.0, 10.0), (1.0, 1.0, 1.0))\n",
      "|   |   >>> Primal evaluation:\n",
      "|   |   | CALL multiply_add_prim(2.0, 2.0, 10.0)\n",
      "|   |   |   | CALL multiply_add_impl(2.0, 2.0, 10.0)\n",
      "|   |   |   | RET  multiply_add_impl = 14.0\n",
      "|   |   | RET  multiply_add_prim = 14.0\n",
      "|   |   >>> Tangent evaluation:\n",
      "|   |   | CALL multiply_add_prim(2.0, 1.0, 1.0)\n",
      "|   |   |   | CALL multiply_add_impl(2.0, 1.0, 1.0)\n",
      "|   |   |   | RET  multiply_add_impl = 3.0\n",
      "|   |   | RET  multiply_add_prim = 3.0\n",
      "|   |   | CALL multiply_add_prim(1.0, 2.0, 3.0)\n",
      "|   |   |   | CALL multiply_add_impl(1.0, 2.0, 3.0)\n",
      "|   |   |   | RET  multiply_add_impl = 5.0\n",
      "|   |   | RET  multiply_add_prim = 5.0\n",
      "|   | RET  multiply_add_value_and_jvp = (14.0, 5.0)\n",
      "| RET  square_add_prim = Tracer<ConcreteArray(14.0, dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "assert jvp(square_add_prim, (2., 10.), (1., 1.)) == (14., 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516a3e99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   | CALL multiply_add_value_and_jvp((Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0), (Tracer<ShapedArray(float32[], weak_type=True)>, Tracer<ShapedArray(float32[], weak_type=True)>, Zero(ShapedArray(float32[], weak_type=True))))\n",
      "|   |   >>> Primal evaluation:\n",
      "|   |   | CALL multiply_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   |   |   | CALL multiply_add_impl(2.0, 2.0, 10.0)\n",
      "|   |   |   | RET  multiply_add_impl = 14.0\n",
      "|   |   | RET  multiply_add_prim = 14.0\n",
      "|   |   >>> Tangent evaluation:\n",
      "|   |   | CALL multiply_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ShapedArray(float32[], weak_type=True)>, 0.0)\n",
      "|   |   |   | CALL multiply_add_abstract_eval(ConcreteArray(2.0, dtype=float32, weak_type=True), ShapedArray(float32[], weak_type=True), ConcreteArray(0.0, dtype=float32, weak_type=True))\n",
      "|   |   |   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "|   |   | RET  multiply_add_prim = Tracer<ShapedArray(float32[])>\n",
      "|   |   | CALL multiply_add_prim(Tracer<ShapedArray(float32[], weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ShapedArray(float32[])>)\n",
      "|   |   |   | CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ConcreteArray(2.0, dtype=float32, weak_type=True), ShapedArray(float32[]))\n",
      "|   |   |   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "|   |   | RET  multiply_add_prim = Tracer<ShapedArray(float32[])>\n",
      "|   | RET  multiply_add_value_and_jvp = (14.0, Tracer<ShapedArray(float32[])>)\n",
      "| RET  square_add_prim = Tracer<ConcreteArray(14.0, dtype=float32)>\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/interpreters/ad.py\", line 258, in get_primitive_transpose\n",
      "    return primitive_transposes[p]\n",
      "KeyError: multiply_add\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 191, in _run_module_as_main\n",
      "    msg = \"%s: %s\" % (sys.executable, exc)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 75, in _run_code\n",
      "    fname = mod_spec.origin\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 12, in <module>\n",
      "    if sys.path[0] == '':\n",
      "jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Transpose rule (for reverse-mode differentiation) for 'multiply_add' not implemented\n",
      "\n",
      "The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n",
      "\n",
      "--------------------\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/2016266372.py\", line 2, in <module>\n",
      "    grad(square_add_prim)(2., 10.)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 165, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 994, in grad_f\n",
      "    _, g = value_and_grad_f(*args, **kwargs)\n",
      "NotImplementedError: Transpose rule (for reverse-mode differentiation) for 'multiply_add' not implemented\n"
     ]
    }
   ],
   "source": [
    "with tracing.ExpectNotImplemented():\n",
    "  grad(square_add_prim)(2., 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ed759ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_transpose(ct, x, y, z):\n",
    "    \"\"\"Evaluates the transpose of a linear primitive.\n",
    "\n",
    "    This method is only used when computing the backward gradient following \n",
    "    value_and_jvp, and is only needed for primitives that are used in the JVP \n",
    "    calculation for some other primitive. We need transposition for multiply_add_prim, \n",
    "    because we have used multiply_add_prim in the computation of the output_tangent in \n",
    "    multiply_add_value_and_jvp.\n",
    "\n",
    "    In our case, multiply_add is not a linear primitive. However, it is used linearly \n",
    "    w.r.t. tangents in multiply_add_value_and_jvp:\n",
    "       output_tangent(xt, yt, zt) = multiply_add_prim(xt, y, multiply_add_prim(x, yt, zt))\n",
    "  \n",
    "    Always one of the first two multiplicative arguments is a constant.\n",
    "    \"\"\"\n",
    "    if not ad.is_undefined_primal(x):\n",
    "        # This use of multiply_add is with a constant \"x\"\n",
    "        assert ad.is_undefined_primal(y)\n",
    "        ct_y = ad.Zero(y.aval) if type(ct) is ad.Zero else multiply_add_prim(x, ct, lax.zeros_like_array(x))\n",
    "        return None, ct_y, ct\n",
    "    else:\n",
    "        # This use of multiply_add is with a constant \"y\"\n",
    "        assert ad.is_undefined_primal(x)\n",
    "        ct_x = ad.Zero(x.aval) if type(ct) is ad.Zero else multiply_add_prim(ct, y, lax.zeros_like_array(y))\n",
    "        return ct_x, None, ct\n",
    "        \n",
    "# Register transpose rule for `multiply_add_p` primitive.\n",
    "ad.primitive_transposes[multiply_add_p] = multiply_add_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b29efef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   | CALL multiply_add_value_and_jvp((Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0), (Tracer<ShapedArray(float32[], weak_type=True)>, Tracer<ShapedArray(float32[], weak_type=True)>, Zero(ShapedArray(float32[], weak_type=True))))\n",
      "|   |   >>> Primal evaluation:\n",
      "|   |   | CALL multiply_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, 10.0)\n",
      "|   |   |   | CALL multiply_add_impl(2.0, 2.0, 10.0)\n",
      "|   |   |   | RET  multiply_add_impl = 14.0\n",
      "|   |   | RET  multiply_add_prim = 14.0\n",
      "|   |   >>> Tangent evaluation:\n",
      "|   |   | CALL multiply_add_prim(Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ShapedArray(float32[], weak_type=True)>, 0.0)\n",
      "|   |   |   | CALL multiply_add_abstract_eval(ConcreteArray(2.0, dtype=float32, weak_type=True), ShapedArray(float32[], weak_type=True), ConcreteArray(0.0, dtype=float32, weak_type=True))\n",
      "|   |   |   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "|   |   | RET  multiply_add_prim = Tracer<ShapedArray(float32[])>\n",
      "|   |   | CALL multiply_add_prim(Tracer<ShapedArray(float32[], weak_type=True)>, Tracer<ConcreteArray(2.0, dtype=float32, weak_type=True)>, Tracer<ShapedArray(float32[])>)\n",
      "|   |   |   | CALL multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ConcreteArray(2.0, dtype=float32, weak_type=True), ShapedArray(float32[]))\n",
      "|   |   |   | RET  multiply_add_abstract_eval = ShapedArray(float32[])\n",
      "|   |   | RET  multiply_add_prim = Tracer<ShapedArray(float32[])>\n",
      "|   | RET  multiply_add_value_and_jvp = (14.0, Tracer<ShapedArray(float32[])>)\n",
      "| RET  square_add_prim = Tracer<ConcreteArray(14.0, dtype=float32)>\n",
      "| CALL multiply_add_transpose(1.0, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), 2.0, UndefinedPrimal(ShapedArray(float32[])))\n",
      "|   | CALL multiply_add_prim(1.0, 2.0, 0.0)\n",
      "|   |   | CALL multiply_add_impl(1.0, 2.0, 0.0)\n",
      "|   |   | RET  multiply_add_impl = 2.0\n",
      "|   | RET  multiply_add_prim = 2.0\n",
      "| RET  multiply_add_transpose = (2.0, None, 1.0)\n",
      "| CALL multiply_add_transpose(1.0, 2.0, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), 0.0)\n",
      "|   | CALL multiply_add_prim(2.0, 1.0, 0.0)\n",
      "|   |   | CALL multiply_add_impl(2.0, 1.0, 0.0)\n",
      "|   |   | RET  multiply_add_impl = 2.0\n",
      "|   | RET  multiply_add_prim = 2.0\n",
      "| RET  multiply_add_transpose = (None, 2.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "assert grad(square_add_prim)(2., 10.) == 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec92850",
   "metadata": {},
   "source": [
    "### RECAP\n",
    "\n",
    "Consider a composition of functions $f(g(h(\\dots(z(x))))$. We normally evaluate such a function from inside-out, first calculate $z(x)$ then use the result as input to the next innermost function until we get to $f$. Similarly for evaluating functions, we decide to compute the derivative from inside-out or outside-in. These are known as forward mode autodiff and backward mode autodiff, respectively.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/fwd_bwd_ad.png\", alt=\"autodiff\" width=\"30%\" />\n",
    "</center>\n",
    "\n",
    "- Is this correct or is it inverted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990052a9",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## PART 3: JAX Primitives and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a633184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ShapedArray(float32[])>, Tracer<ShapedArray(float32[])>)\n",
      "\n",
      "Found expected exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6900/44637082.py\", line 6, in <module>\n",
      "    vmap(square_add_prim, in_axes=0, out_axes=0)(a, b)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 165, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/scratch/research/notebooks/venv/lib/python3.10/site-packages/jax/_src/api.py\", line 1555, in batched_fun\n",
      "    out_flat = batching.batch(\n",
      "NotImplementedError: Batching rule for 'multiply_add' not implemented\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2., 3.])\n",
    "b = np.array([10., 20.])\n",
    "\n",
    "# The arguments are two vectors instead of two scalars\n",
    "with tracing.ExpectNotImplemented():\n",
    "    vmap(square_add_prim, in_axes=0, out_axes=0)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05eb5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracing.log_calls\n",
    "def multiply_add_batch(vector_arg_values, batch_axes):\n",
    "    \"\"\"Computes the batched version of the primitive.\n",
    "\n",
    "    This must be a JAX-traceable function.\n",
    "\n",
    "    Since the multiply_add primitive already operates pointwise on arbitrary\n",
    "    dimension tensors, to batch it we can use the primitive itself. This works as\n",
    "    long as both the inputs have the same dimensions and are batched along the\n",
    "    same axes. The result is batched along the axis that the inputs are batched.\n",
    "\n",
    "    Args:\n",
    "    vector_arg_values: a tuple of two arguments, each being a tensor of matching\n",
    "      shape.\n",
    "    batch_axes: the axes that are being batched. See vmap documentation.\n",
    "    Returns:\n",
    "    a tuple of the result, and the result axis that was batched. \n",
    "    \"\"\"\n",
    "    assert batch_axes[0] == batch_axes[1]\n",
    "    assert batch_axes[0] == batch_axes[2]\n",
    "    tracing.log(\">>> Using multiply_add to compute the batch\")\n",
    "    res = multiply_add_prim(*vector_arg_values)\n",
    "    return res, batch_axes[0]\n",
    "\n",
    "\n",
    "batching.primitive_batchers[multiply_add_p] = multiply_add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d85310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| CALL square_add_prim(Tracer<ShapedArray(float32[])>, Tracer<ShapedArray(float32[])>)\n",
      "|   | CALL multiply_add_batch(([2. 3.], [2. 3.], [10. 20.]), (0, 0, 0))\n",
      "|   |   >>> Using multiply_add to compute the batch\n",
      "|   |   | CALL multiply_add_prim([2. 3.], [2. 3.], [10. 20.])\n",
      "|   |   |   | CALL multiply_add_impl([2. 3.], [2. 3.], [10. 20.])\n",
      "|   |   |   | RET  multiply_add_impl = [14. 29.]\n",
      "|   |   | RET  multiply_add_prim = [14. 29.]\n",
      "|   | RET  multiply_add_batch = ([14. 29.], 0)\n",
      "| RET  square_add_prim = Tracer<ShapedArray(float32[])>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14., 29.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(square_add_prim, in_axes=0, out_axes=0)(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4457a03",
   "metadata": {},
   "source": [
    "### RECAP\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/batching.png\" alt=\"batching\" width=\"50%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d509a6a",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## PART 4: Summary\n",
    "\n",
    "1. Create a primitive:\n",
    "\n",
    "```python\n",
    "my_primitive_p = Primitive(\"my_primitive\")\n",
    "```\n",
    "\n",
    "2. Define a concrete evaluation rule for interpretation:\n",
    "\n",
    "```python\n",
    "my_primitive_p.def_impl(my_primitive_impl)\n",
    "```\n",
    "\n",
    "3. Define abstract evaluation rule for JIT compilation:\n",
    "\n",
    "```python\n",
    "my_primitive_p.def_abstract_eval(my_primitive_abs_eval)\n",
    "```\n",
    "\n",
    "4. Define XLA translation rule for JIT compilation:\n",
    "\n",
    "```python\n",
    "jax.interpreters.xla.register_translation(\n",
    "    my_primitive_p,\n",
    "    my_primitive_xla_compile_gpu,\n",
    "    platform='gpu')\n",
    "```\n",
    "\n",
    "5. Define Jacobian-Vector Product (JVP) rule for forward AD:\n",
    "\n",
    "```python\n",
    "jax.interpreters.ad.primitive_jvps[my_primitive_p] = my_primitive_value_and_jvp\n",
    "```\n",
    "\n",
    "6. Define transpose rule for backward AD:\n",
    "\n",
    "```python\n",
    "jax.interpreters.ad.primitive_transposes[my_primitive_p] = my_primitive_transpose\n",
    "```\n",
    "\n",
    "7. Define batching rule:\n",
    "\n",
    "```python\n",
    "jax.interpreters.batching.primitive_batchers[my_primitive_p] = my_primitive_batch\n",
    "```\n",
    "\n",
    "### JAX Core Reference\n",
    "\n",
    "This is an attempt of a more or less complete picture of the core classes of JAX.\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/jax_core_full.png\" alt=\"\" width=\"100%\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
