{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4127142a",
   "metadata": {},
   "source": [
    "# 5.4. XLA and PJIT\n",
    "\n",
    "> **Author**: Gustavo Leite / **Date**: March 2022.\n",
    "\n",
    "In this lecture we will take a look at the XLA compiler that compiles JAX functions. XLA is also developed by Google and lives in the Tensorflow repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180d7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "\n",
    "from jax import jit, grad, make_jaxpr, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f158e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLA will see 2 logical CPUs instead of one.\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79850695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0),\n",
       " CpuDevice(id=1),\n",
       " CpuDevice(id=2),\n",
       " CpuDevice(id=3),\n",
       " CpuDevice(id=4),\n",
       " CpuDevice(id=5),\n",
       " CpuDevice(id=6),\n",
       " CpuDevice(id=7)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce006ea",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## XLA Compiler and IR\n",
    "\n",
    "<center>\n",
    "    <br />\n",
    "    <img src=\"images/flows.png\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ae0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14cee3",
   "metadata": {},
   "source": [
    "### JAXPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07bc4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = neg a\n",
       "    c\u001b[35m:f32[]\u001b[39m = exp b\n",
       "    d\u001b[35m:f32[]\u001b[39m = add c 1.0\n",
       "    e\u001b[35m:f32[]\u001b[39m = div 1.0 d\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(sigmoid)(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec076151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[10]\u001b[39m = neg a\n",
       "    c\u001b[35m:f32[10]\u001b[39m = exp b\n",
       "    d\u001b[35m:f32[10]\u001b[39m = add c 1.0\n",
       "    e\u001b[35m:f32[10]\u001b[39m = div 1.0 d\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jnp.ones(10)\n",
    "make_jaxpr(sigmoid)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e60719e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = xla_call[\n",
       "      call_jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; c\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "          \u001b[39m\u001b[22m\u001b[22md\u001b[35m:f32[]\u001b[39m = neg c\n",
       "          e\u001b[35m:f32[]\u001b[39m = exp d\n",
       "          f\u001b[35m:f32[]\u001b[39m = add e 1.0\n",
       "          g\u001b[35m:f32[]\u001b[39m = div 1.0 f\n",
       "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(g,) }\n",
       "      name=sigmoid\n",
       "    ] a\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(b,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(jit(sigmoid))(1.0)\n",
    "#          ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b96235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = neg a\n",
       "    c\u001b[35m:f32[]\u001b[39m = exp b\n",
       "    d\u001b[35m:f32[]\u001b[39m = add c 1.0\n",
       "    _\u001b[35m:f32[]\u001b[39m = div 1.0 d\n",
       "    e\u001b[35m:f32[]\u001b[39m = integer_pow[y=-2] d\n",
       "    f\u001b[35m:f32[]\u001b[39m = mul 1.0 e\n",
       "    g\u001b[35m:f32[]\u001b[39m = mul f 1.0\n",
       "    h\u001b[35m:f32[]\u001b[39m = neg g\n",
       "    i\u001b[35m:f32[]\u001b[39m = mul h c\n",
       "    j\u001b[35m:f32[]\u001b[39m = neg i\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(grad(sigmoid))(1.0)\n",
    "#          ============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0597c44",
   "metadata": {},
   "source": [
    "### HLO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6246d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = <class 'jaxlib.xla_extension.XlaComputation'>\n",
      "======================================================================\n",
      "HloModule jit_sigmoid.2\n",
      "\n",
      "ENTRY main.8 {\n",
      "  constant.2 = f32[] constant(1)\n",
      "  broadcast.3 = f32[10]{0} broadcast(constant.2), dimensions={}\n",
      "  Arg_0.1 = f32[10]{0} parameter(0)\n",
      "  negate.4 = f32[10]{0} negate(Arg_0.1)\n",
      "  exponential.5 = f32[10]{0} exponential(negate.4)\n",
      "  add.6 = f32[10]{0} add(exponential.5, broadcast.3)\n",
      "  ROOT divide.7 = f32[10]{0} divide(broadcast.3, add.6)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoid_jit = jit(sigmoid)\n",
    "x = np.ones(10)\n",
    "\n",
    "ir = sigmoid_jit.lower(x).compiler_ir('hlo')\n",
    "#                =====      ===========\n",
    "\n",
    "print(f\"Type = {type(ir)}\")\n",
    "print(\"=\" * 70)\n",
    "print(ir.as_hlo_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15a15641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'lower'\n",
      " Note: Lowering is only available when the function is compiled!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ir = grad(sigmoid).lower(1.0).compiler_ir('hlo')\n",
    "except AttributeError as error:\n",
    "    print(f\"Error: {error}\", file=sys.stderr)\n",
    "    print(f\" Note: Lowering is only available when the function is compiled!\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee12774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = <class 'jaxlib.xla_extension.XlaComputation'>\n",
      "======================================================================\n",
      "HloModule jit_sigmoid.3\n",
      "\n",
      "ENTRY main.11 {\n",
      "  constant.2 = f32[] constant(1)\n",
      "  Arg_0.1 = f32[] parameter(0)\n",
      "  negate.3 = f32[] negate(Arg_0.1)\n",
      "  exponential.4 = f32[] exponential(negate.3)\n",
      "  add.5 = f32[] add(exponential.4, constant.2)\n",
      "  multiply.6 = f32[] multiply(add.5, add.5)\n",
      "  divide.7 = f32[] divide(constant.2, multiply.6)\n",
      "  negate.8 = f32[] negate(divide.7)\n",
      "  multiply.9 = f32[] multiply(negate.8, exponential.4)\n",
      "  ROOT negate.10 = f32[] negate(multiply.9)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ir = jit(grad(sigmoid)).lower(1.0).compiler_ir('hlo')\n",
    "#    ==================\n",
    "\n",
    "print(f\"Type = {type(ir)}\")\n",
    "print(\"=\" * 70)\n",
    "print(ir.as_hlo_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd48da2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = neg a\n",
       "    c\u001b[35m:f32[]\u001b[39m = exp b\n",
       "    d\u001b[35m:f32[]\u001b[39m = add c 1.0\n",
       "    _\u001b[35m:f32[]\u001b[39m = div 1.0 d\n",
       "    e\u001b[35m:f32[]\u001b[39m = integer_pow[y=-2] d\n",
       "    f\u001b[35m:f32[]\u001b[39m = mul 1.0 e\n",
       "    g\u001b[35m:f32[]\u001b[39m = mul f 1.0\n",
       "    h\u001b[35m:f32[]\u001b[39m = neg g\n",
       "    i\u001b[35m:f32[]\u001b[39m = mul h c\n",
       "    j\u001b[35m:f32[]\u001b[39m = neg i\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(grad(sigmoid))(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f43c87",
   "metadata": {},
   "source": [
    "### MHLO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ebf651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = <class 'jaxlib.mlir._mlir_libs._mlir.ir.Module'>\n",
      "======================================================================\n",
      "module @jit_sigmoid.5 {\n",
      "  func public @main(%arg0: tensor<10xf32>) -> tensor<10xf32> {\n",
      "    %0 = mhlo.negate %arg0 : tensor<10xf32>\n",
      "    %1 = mhlo.exponential %0 : tensor<10xf32>\n",
      "    %2 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %3 = \"mhlo.broadcast_in_dim\"(%2) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32>\n",
      "    %4 = mhlo.add %1, %3 : tensor<10xf32>\n",
      "    %5 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %6 = \"mhlo.broadcast_in_dim\"(%5) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32>\n",
      "    %7 = mhlo.divide %6, %4 : tensor<10xf32>\n",
      "    return %7 : tensor<10xf32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "ir = jit(sigmoid).lower(x).compiler_ir('mhlo')\n",
    "\n",
    "print(f\"Type = {type(ir)}\")\n",
    "print(\"=\" * 70)\n",
    "print(ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5272bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = <class 'jaxlib.mlir._mlir_libs._mlir.ir.Module'>\n",
      "======================================================================\n",
      "module @jit_sigmoid.6 {\n",
      "  func public @main(%arg0: tensor<f32>) -> tensor<f32> {\n",
      "    %0 = mhlo.negate %arg0 : tensor<f32>\n",
      "    %1 = mhlo.exponential %0 : tensor<f32>\n",
      "    %2 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %3 = mhlo.add %1, %2 : tensor<f32>\n",
      "    %4 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %5 = mhlo.divide %4, %3 : tensor<f32>\n",
      "    %6 = mhlo.multiply %3, %3 : tensor<f32>\n",
      "    %7 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %8 = mhlo.divide %7, %6 : tensor<f32>\n",
      "    %9 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %10 = mhlo.multiply %9, %8 : tensor<f32>\n",
      "    %11 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %12 = mhlo.multiply %10, %11 : tensor<f32>\n",
      "    %13 = mhlo.negate %12 : tensor<f32>\n",
      "    %14 = mhlo.multiply %13, %1 : tensor<f32>\n",
      "    %15 = mhlo.negate %14 : tensor<f32>\n",
      "    return %15 : tensor<f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ir = jit(grad(sigmoid)).lower(1.0).compiler_ir('mhlo')\n",
    "\n",
    "print(f\"Type = {type(ir)}\")\n",
    "print(\"=\" * 70)\n",
    "print(ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "003af3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = <class 'jaxlib.mlir._mlir_libs._mlir.ir.Module'>\n",
      "======================================================================\n",
      "module @jit_sigmoid.12 {\n",
      "  func public @main(%arg0: tensor<2x2xf32>) -> tensor<2x2xf32> {\n",
      "    %0 = mhlo.negate %arg0 : tensor<2x2xf32>\n",
      "    %1 = mhlo.exponential %0 : tensor<2x2xf32>\n",
      "    %2 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %3 = \"mhlo.broadcast_in_dim\"(%2) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<2x2xf32>\n",
      "    %4 = mhlo.add %1, %3 : tensor<2x2xf32>\n",
      "    %5 = mhlo.constant dense<1.000000e+00> : tensor<f32>\n",
      "    %6 = \"mhlo.broadcast_in_dim\"(%5) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<2x2xf32>\n",
      "    %7 = mhlo.divide %6, %4 : tensor<2x2xf32>\n",
      "    return %7 : tensor<2x2xf32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ir = jit(sigmoid).lower(jnp.eye(2)).compiler_ir('mhlo')\n",
    "#                      ==========\n",
    "\n",
    "print(f\"Type = {type(ir)}\")\n",
    "print(\"=\" * 70)\n",
    "print(ir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32b8d9",
   "metadata": {},
   "source": [
    "### Revisiting our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96964e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):   \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bb45ed9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule jit_apply.63\n",
      "\n",
      "relu.23 {\n",
      "  Arg_0.24 = f32[28,28,32]{2,1,0} parameter(0)\n",
      "  constant.25 = f32[] constant(0)\n",
      "  broadcast.26 = f32[28,28,32]{2,1,0} broadcast(constant.25), dimensions={}\n",
      "  ROOT maximum.27 = f32[28,28,32]{2,1,0} maximum(Arg_0.24, broadcast.26)\n",
      "}\n",
      "\n",
      "region_0.30 {\n",
      "  Arg_0.31 = f32[] parameter(0)\n",
      "  Arg_1.32 = f32[] parameter(1)\n",
      "  ROOT add.33 = f32[] add(Arg_0.31, Arg_1.32)\n",
      "}\n",
      "\n",
      "relu_0.45 {\n",
      "  Arg_0.46 = f32[14,14,64]{2,1,0} parameter(0)\n",
      "  constant.47 = f32[] constant(0)\n",
      "  broadcast.48 = f32[14,14,64]{2,1,0} broadcast(constant.47), dimensions={}\n",
      "  ROOT maximum.49 = f32[14,14,64]{2,1,0} maximum(Arg_0.46, broadcast.48)\n",
      "}\n",
      "\n",
      "region_1.52 {\n",
      "  Arg_0.53 = f32[] parameter(0)\n",
      "  Arg_1.54 = f32[] parameter(1)\n",
      "  ROOT add.55 = f32[] add(Arg_0.53, Arg_1.54)\n",
      "}\n",
      "\n",
      "relu_1.66 {\n",
      "  Arg_0.67 = f32[7,256]{1,0} parameter(0)\n",
      "  constant.68 = f32[] constant(0)\n",
      "  broadcast.69 = f32[7,256]{1,0} broadcast(constant.68), dimensions={}\n",
      "  ROOT maximum.70 = f32[7,256]{1,0} maximum(Arg_0.67, broadcast.69)\n",
      "}\n",
      "\n",
      "region_2.78 {\n",
      "  Arg_0.79 = f32[] parameter(0)\n",
      "  Arg_1.80 = f32[] parameter(1)\n",
      "  ROOT maximum.81 = f32[] maximum(Arg_0.79, Arg_1.80)\n",
      "}\n",
      "\n",
      "region_3.82 {\n",
      "  Arg_0.83 = f32[] parameter(0)\n",
      "  Arg_1.84 = f32[] parameter(1)\n",
      "  ROOT add.85 = f32[] add(Arg_0.83, Arg_1.84)\n",
      "}\n",
      "\n",
      "log_softmax.86 {\n",
      "  Arg_0.87 = f32[7,10]{1,0} parameter(0)\n",
      "  constant.89 = f32[] constant(-inf)\n",
      "  reduce.90 = f32[7]{0} reduce(Arg_0.87, constant.89), dimensions={1}, to_apply=region_2.78\n",
      "  reshape.91 = f32[7,1]{1,0} reshape(reduce.90)\n",
      "  broadcast.92 = f32[7,1]{1,0} broadcast(reshape.91), dimensions={0,1}\n",
      "  reshape.93 = f32[7]{0} reshape(broadcast.92)\n",
      "  broadcast.94 = f32[7,10]{1,0} broadcast(reshape.93), dimensions={0}\n",
      "  subtract.95 = f32[7,10]{1,0} subtract(Arg_0.87, broadcast.94)\n",
      "  exponential.96 = f32[7,10]{1,0} exponential(subtract.95)\n",
      "  constant.88 = f32[] constant(0)\n",
      "  reduce.97 = f32[7]{0} reduce(exponential.96, constant.88), dimensions={1}, to_apply=region_3.82\n",
      "  reshape.98 = f32[7,1]{1,0} reshape(reduce.97)\n",
      "  log.99 = f32[7,1]{1,0} log(reshape.98)\n",
      "  broadcast.100 = f32[7,1]{1,0} broadcast(log.99), dimensions={0,1}\n",
      "  reshape.101 = f32[7]{0} reshape(broadcast.100)\n",
      "  broadcast.102 = f32[7,10]{1,0} broadcast(reshape.101), dimensions={0}\n",
      "  ROOT subtract.103 = f32[7,10]{1,0} subtract(subtract.95, broadcast.102)\n",
      "}\n",
      "\n",
      "ENTRY main.105 {\n",
      "  Arg_8.9 = f32[28,28,1]{2,1,0} parameter(8)\n",
      "  reshape.15 = f32[1,28,28,1]{3,2,1,0} reshape(Arg_8.9)\n",
      "  Arg_1.2 = f32[3,3,1,32]{3,2,1,0} parameter(1)\n",
      "  convolution.16 = f32[1,28,28,32]{3,2,1,0} convolution(reshape.15, Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n",
      "  Arg_0.1 = f32[32]{0} parameter(0)\n",
      "  reshape.17 = f32[1,1,1,32]{3,2,1,0} reshape(Arg_0.1)\n",
      "  broadcast.18 = f32[1,1,1,32]{3,2,1,0} broadcast(reshape.17), dimensions={0,1,2,3}\n",
      "  reshape.19 = f32[1,32]{1,0} reshape(broadcast.18)\n",
      "  broadcast.20 = f32[1,28,28,32]{3,2,1,0} broadcast(reshape.19), dimensions={0,3}\n",
      "  add.21 = f32[1,28,28,32]{3,2,1,0} add(convolution.16, broadcast.20)\n",
      "  reshape.22 = f32[28,28,32]{2,1,0} reshape(add.21)\n",
      "  call.28 = f32[28,28,32]{2,1,0} call(reshape.22), to_apply=relu.23\n",
      "  reshape.29 = f32[1,28,28,32]{3,2,1,0} reshape(call.28)\n",
      "  constant.12 = f32[] constant(0)\n",
      "  reduce-window.34 = f32[1,14,14,32]{3,2,1,0} reduce-window(reshape.29, constant.12), window={size=1x2x2x1 stride=1x2x2x1}, to_apply=region_0.30\n",
      "  reshape.35 = f32[14,14,32]{2,1,0} reshape(reduce-window.34)\n",
      "  constant.13 = f32[] constant(4)\n",
      "  broadcast.14 = f32[14,14,32]{2,1,0} broadcast(constant.13), dimensions={}\n",
      "  divide.36 = f32[14,14,32]{2,1,0} divide(reshape.35, broadcast.14)\n",
      "  reshape.37 = f32[1,14,14,32]{3,2,1,0} reshape(divide.36)\n",
      "  Arg_3.4 = f32[3,3,32,64]{3,2,1,0} parameter(3)\n",
      "  convolution.38 = f32[1,14,14,64]{3,2,1,0} convolution(reshape.37, Arg_3.4), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n",
      "  Arg_2.3 = f32[64]{0} parameter(2)\n",
      "  reshape.39 = f32[1,1,1,64]{3,2,1,0} reshape(Arg_2.3)\n",
      "  broadcast.40 = f32[1,1,1,64]{3,2,1,0} broadcast(reshape.39), dimensions={0,1,2,3}\n",
      "  reshape.41 = f32[1,64]{1,0} reshape(broadcast.40)\n",
      "  broadcast.42 = f32[1,14,14,64]{3,2,1,0} broadcast(reshape.41), dimensions={0,3}\n",
      "  add.43 = f32[1,14,14,64]{3,2,1,0} add(convolution.38, broadcast.42)\n",
      "  reshape.44 = f32[14,14,64]{2,1,0} reshape(add.43)\n",
      "  call.50 = f32[14,14,64]{2,1,0} call(reshape.44), to_apply=relu_0.45\n",
      "  reshape.51 = f32[1,14,14,64]{3,2,1,0} reshape(call.50)\n",
      "  reduce-window.56 = f32[1,7,7,64]{3,2,1,0} reduce-window(reshape.51, constant.12), window={size=1x2x2x1 stride=1x2x2x1}, to_apply=region_1.52\n",
      "  reshape.57 = f32[7,7,64]{2,1,0} reshape(reduce-window.56)\n",
      "  constant.10 = f32[] constant(4)\n",
      "  broadcast.11 = f32[7,7,64]{2,1,0} broadcast(constant.10), dimensions={}\n",
      "  divide.58 = f32[7,7,64]{2,1,0} divide(reshape.57, broadcast.11)\n",
      "  reshape.59 = f32[7,448]{1,0} reshape(divide.58)\n",
      "  Arg_5.6 = f32[448,256]{1,0} parameter(5)\n",
      "  dot.60 = f32[7,256]{1,0} dot(reshape.59, Arg_5.6), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  Arg_4.5 = f32[256]{0} parameter(4)\n",
      "  reshape.61 = f32[1,256]{1,0} reshape(Arg_4.5)\n",
      "  broadcast.62 = f32[1,256]{1,0} broadcast(reshape.61), dimensions={0,1}\n",
      "  reshape.63 = f32[256]{0} reshape(broadcast.62)\n",
      "  broadcast.64 = f32[7,256]{1,0} broadcast(reshape.63), dimensions={1}\n",
      "  add.65 = f32[7,256]{1,0} add(dot.60, broadcast.64)\n",
      "  call.71 = f32[7,256]{1,0} call(add.65), to_apply=relu_1.66\n",
      "  Arg_7.8 = f32[256,10]{1,0} parameter(7)\n",
      "  dot.72 = f32[7,10]{1,0} dot(call.71, Arg_7.8), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  Arg_6.7 = f32[10]{0} parameter(6)\n",
      "  reshape.73 = f32[1,10]{1,0} reshape(Arg_6.7)\n",
      "  broadcast.74 = f32[1,10]{1,0} broadcast(reshape.73), dimensions={0,1}\n",
      "  reshape.75 = f32[10]{0} reshape(broadcast.74)\n",
      "  broadcast.76 = f32[7,10]{1,0} broadcast(reshape.75), dimensions={1}\n",
      "  add.77 = f32[7,10]{1,0} add(dot.72, broadcast.76)\n",
      "  ROOT call.104 = f32[7,10]{1,0} call(add.77), to_apply=log_softmax.86\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key    = random.PRNGKey(0)\n",
    "image  = jnp.zeros((28, 28, 1))\n",
    "model  = CNN()\n",
    "params = model.init(key, image)\n",
    "apply  = jit(model.apply)\n",
    "\n",
    "ir = apply.lower(params, image).compiler_ir(\"hlo\")\n",
    "#                =============\n",
    "\n",
    "print(ir.as_hlo_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f6141",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Peeking under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19166378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.XlaComputation'>\n",
      "======================================================================\n",
      "HloModule xla_computation_dot.65\n",
      "\n",
      "ENTRY main.5 {\n",
      "  Arg_0.1 = f32[2,2]{1,0} parameter(0)\n",
      "  Arg_1.2 = f32[2,2]{1,0} parameter(1)\n",
      "  dot.3 = f32[2,2]{1,0} dot(Arg_0.1, Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  ROOT tuple.4 = (f32[2,2]{1,0}) tuple(dot.3)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x2 matrix\n",
    "M = jnp.zeros((2, 2))\n",
    "\n",
    "# Create XLA module from the \"dot\" operator\n",
    "computation = jax.xla_computation(jnp.dot)(M, M)\n",
    "\n",
    "# Print HLO\n",
    "print(type(computation))\n",
    "print(\"=\" * 70)\n",
    "print(computation.as_hlo_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c072cf",
   "metadata": {},
   "source": [
    "### CPU\n",
    "\n",
    "When compiling for the CPU, notice how the compiler simply used the `dot` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbe37a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule xla_computation_dot.65\n",
      "\n",
      "ENTRY %main.5 (Arg_0.1: f32[2,2], Arg_1.2: f32[2,2]) -> (f32[2,2]) {\n",
      "  %Arg_0.1 = f32[2,2]{1,0} parameter(0)\n",
      "  %Arg_1.2 = f32[2,2]{1,0} parameter(1)\n",
      "  %dot.3 = f32[2,2]{1,0} dot(f32[2,2]{1,0} %Arg_0.1, f32[2,2]{1,0} %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"xla_computation(dot)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4480/1430351131.py\" source_line=5}\n",
      "  ROOT %tuple.4 = (f32[2,2]{1,0}) tuple(f32[2,2]{1,0} %dot.3)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cpu_backend = jax.lib.xla_bridge.get_backend('cpu')\n",
    "executable = cpu_backend.compile(computation)\n",
    "module = executable.hlo_modules()[0]\n",
    "\n",
    "print(module.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb18e1",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "However, when compiling for the GPU, the compiler delegated the work to the CuBLAS library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e90fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule xla_computation_dot.65\n",
      "\n",
      "ENTRY %main.5 (Arg_0.1: f32[2,2], Arg_1.2: f32[2,2]) -> (f32[2,2]) {\n",
      "  %Arg_0.1 = f32[2,2]{1,0} parameter(0)\n",
      "  %Arg_1.2 = f32[2,2]{1,0} parameter(1)\n",
      "  %cublas-gemm.1 = f32[2,2]{1,0} custom-call(f32[2,2]{1,0} %Arg_0.1, f32[2,2]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"xla_computation(dot)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4480/1430351131.py\" source_line=5}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"selected_algorithm\\\":\\\"9\\\"}\"\n",
      "  ROOT %tuple.4 = (f32[2,2]{1,0}) tuple(f32[2,2]{1,0} %cublas-gemm.1)\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 08:50:23.664881: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.4\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    }
   ],
   "source": [
    "gpu_backend = jax.lib.xla_bridge.get_backend('gpu')\n",
    "executable = gpu_backend.compile(computation)\n",
    "module = executable.hlo_modules()[0]\n",
    "\n",
    "print(module.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6680c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule xla_computation_sigmoid.67\n",
      "\n",
      "%fused_computation (param_0.4: f32[]) -> f32[] {\n",
      "  %constant.1 = f32[] constant(1)\n",
      "  %param_0.4 = f32[] parameter(0)\n",
      "  %negate.6 = f32[] negate(f32[] %param_0.4), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/neg\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %exponential.1 = f32[] exponential(f32[] %negate.6), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/exp\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %add.1 = f32[] add(f32[] %exponential.1, f32[] %constant.1), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/add\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %multiply.3 = f32[] multiply(f32[] %add.1, f32[] %add.1), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/mul\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %divide.1 = f32[] divide(f32[] %constant.1, f32[] %multiply.3), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/div\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %negate.4 = f32[] negate(f32[] %divide.1), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/neg\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  %multiply.2 = f32[] multiply(f32[] %negate.4, f32[] %exponential.1), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/mul\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  ROOT %negate.3 = f32[] negate(f32[] %multiply.2), metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/neg\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "}\n",
      "\n",
      "ENTRY %main.15 (Arg_0.1: f32[]) -> (f32[]) {\n",
      "  %Arg_0.1 = f32[] parameter(0)\n",
      "  %fusion = f32[] fusion(f32[] %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"xla_computation(sigmoid)/jit(main)/jit(sigmoid)/neg\" source_file=\"/tmp/ipykernel_4480/1433209252.py\" source_line=2}\n",
      "  ROOT %tuple.14 = (f32[]) tuple(f32[] %fusion)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "computation = jax.xla_computation(jit(grad(sigmoid)))(1.0)\n",
    "\n",
    "gpu_backend = jax.lib.xla_bridge.get_backend('cpu')\n",
    "executable = gpu_backend.compile(computation)\n",
    "module = executable.hlo_modules()[0]\n",
    "\n",
    "print(module.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e425958",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Advanced Partitioning\n",
    "\n",
    "In this section we will look how JAX enables data partitioning across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a359923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0),\n",
       " CpuDevice(id=1),\n",
       " CpuDevice(id=2),\n",
       " CpuDevice(id=3),\n",
       " CpuDevice(id=4),\n",
       " CpuDevice(id=5),\n",
       " CpuDevice(id=6),\n",
       " CpuDevice(id=7)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f9b3c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Revisiting PMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69064079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.ones((8, 4))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b04eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = ShardedDeviceArray([4., 4., 4., 4., 4., 4., 4., 4.], dtype=float32)\n",
      "\n",
      "Device 0: DeviceArray(4., dtype=float32)\n",
      "Device 1: DeviceArray(4., dtype=float32)\n",
      "Device 2: DeviceArray(4., dtype=float32)\n",
      "Device 3: DeviceArray(4., dtype=float32)\n",
      "Device 4: DeviceArray(4., dtype=float32)\n",
      "Device 5: DeviceArray(4., dtype=float32)\n",
      "Device 6: DeviceArray(4., dtype=float32)\n",
      "Device 7: DeviceArray(4., dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = jax.pmap(jnp.sum, in_axes=0)(data)\n",
    "\n",
    "print(f\"Result = {result!r}\\n\")\n",
    "for i, buffer in enumerate(result.device_buffers):\n",
    "    print(f\"Device {i}: {buffer!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfbf3ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = ShardedDeviceArray([8., 8., 8., 8.], dtype=float32)\n",
      "\n",
      "Device 0: DeviceArray(8., dtype=float32)\n",
      "Device 1: DeviceArray(8., dtype=float32)\n",
      "Device 2: DeviceArray(8., dtype=float32)\n",
      "Device 3: DeviceArray(8., dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = jax.pmap(jnp.sum, in_axes=1)(data)\n",
    "\n",
    "print(f\"Result = {result!r}\\n\")\n",
    "for i, buffer in enumerate(result.device_buffers):\n",
    "    print(f\"Device {i}: {buffer!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e0b13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Getting Started with PJIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82ad288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import PartitionSpec\n",
    "from jax.experimental.maps import Mesh\n",
    "from jax.experimental.pjit import pjit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4ad01e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[CpuDevice(id=0), CpuDevice(id=1)],\n",
       "       [CpuDevice(id=2), CpuDevice(id=3)],\n",
       "       [CpuDevice(id=4), CpuDevice(id=5)],\n",
       "       [CpuDevice(id=6), CpuDevice(id=7)]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_array = np.asarray(devices).reshape((4, 2))\n",
    "device_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55fbece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE MESH:\n",
      "Mesh(array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]), ('x', 'y'))\n",
      "\n",
      "DATA ARRAY:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]]\n"
     ]
    }
   ],
   "source": [
    "# Create a device mesh\n",
    "# ------------------------------------------\n",
    "device_mesh = Mesh(device_array, (\"x\", \"y\"))\n",
    "\n",
    "print(\"DEVICE MESH:\")\n",
    "print(device_mesh)\n",
    "print()\n",
    "\n",
    "# Create 8x4 matrix\n",
    "# ------------------------------------------\n",
    "data = jnp.arange(8 * 4).reshape(8, 4)\n",
    "\n",
    "print(\"DATA ARRAY:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c06343",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/pjit_data_devices.png\" width=\"50%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5177524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = pjit(\n",
    "        lambda x: x,                               # The function to be transformed\n",
    "        in_axis_resources=None,                    # How the inputs are partitioned\n",
    "        out_axis_resources=PartitionSpec(\"x\", \"y\") # How the outputs are partitioned\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a45d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = 'ShardedDeviceArray'\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]]\n",
      "--------------------------------------------------------------------------------\n",
      "Device 0 has buffer:\n",
      "[[0 1]\n",
      " [4 5]]\n",
      "\n",
      "Device 1 has buffer:\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "\n",
      "Device 2 has buffer:\n",
      "[[ 8  9]\n",
      " [12 13]]\n",
      "\n",
      "Device 3 has buffer:\n",
      "[[10 11]\n",
      " [14 15]]\n",
      "\n",
      "Device 4 has buffer:\n",
      "[[16 17]\n",
      " [20 21]]\n",
      "\n",
      "Device 5 has buffer:\n",
      "[[18 19]\n",
      " [22 23]]\n",
      "\n",
      "Device 6 has buffer:\n",
      "[[24 25]\n",
      " [28 29]]\n",
      "\n",
      "Device 7 has buffer:\n",
      "[[26 27]\n",
      " [30 31]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with device_mesh:\n",
    "    output = fn(data)\n",
    "    \n",
    "print(f\"Type = {type(output).__name__!r}\")\n",
    "print(output)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, buffer in enumerate(output.device_buffers):\n",
    "    print(f\"Device {i} has buffer:\\n{buffer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c12ab0",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/pjit_partitioning.png\" width=\"100%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658160d",
   "metadata": {},
   "source": [
    "### What does PJIT HLO look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5aaa4cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule pjit_square.75\n",
      "\n",
      "ENTRY main.5 {\n",
      "  Arg_0.1 = s32[8,4]{1,0} parameter(0), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "  multiply.2 = s32[8,4]{1,0} multiply(Arg_0.1, Arg_0.1)\n",
      "  tuple.3 = (s32[8,4]{1,0}) tuple(multiply.2)\n",
      "  ROOT get-tuple-element.4 = s32[8,4]{1,0} get-tuple-element(tuple.3), index=0, sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "fn = pjit(\n",
    "        square,\n",
    "        in_axis_resources=PartitionSpec(\"x\", \"y\"),\n",
    "        out_axis_resources=PartitionSpec(\"x\", \"y\")\n",
    ")\n",
    "\n",
    "with device_mesh:\n",
    "    ir = fn.lower(data).compiler_ir('hlo')\n",
    "\n",
    "print(ir.as_hlo_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e01e8",
   "metadata": {},
   "source": [
    "### Functions with multiple parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7f6382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]]\n"
     ]
    }
   ],
   "source": [
    "M = jnp.eye(8)\n",
    "v = jnp.arange(8).reshape((8, 1))\n",
    "\n",
    "print(M)\n",
    "print()\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67854e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule pjit_dot.83\n",
      "\n",
      "ENTRY main.7 {\n",
      "  Arg_0.1 = f32[8,8]{1,0} parameter(0), sharding={devices=[8,1]0,1,2,3,4,5,6,7}\n",
      "  Arg_1.2 = s32[8,1]{1,0} parameter(1), sharding={replicated}\n",
      "  convert.3 = f32[8,1]{1,0} convert(Arg_1.2)\n",
      "  dot.4 = f32[8,1]{1,0} dot(Arg_0.1, convert.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  tuple.5 = (f32[8,1]{1,0}) tuple(dot.4)\n",
      "  ROOT get-tuple-element.6 = f32[8,1]{1,0} get-tuple-element(tuple.5), index=0, sharding={devices=[8,1]0,1,2,3,4,5,6,7}\n",
      "}\n",
      "\n",
      "\n",
      "0 [[0.]]\n",
      "1 [[1.]]\n",
      "2 [[2.]]\n",
      "3 [[3.]]\n",
      "4 [[4.]]\n",
      "5 [[5.]]\n",
      "6 [[6.]]\n",
      "7 [[7.]]\n"
     ]
    }
   ],
   "source": [
    "spec = PartitionSpec((\"x\", \"y\"), None)\n",
    "\n",
    "f = pjit(jnp.dot,\n",
    "         in_axis_resources=(spec, None),\n",
    "         out_axis_resources=spec)\n",
    "\n",
    "with device_mesh:\n",
    "    ir = f.lower(M, v).compiler_ir(\"hlo\")\n",
    "    output = f(M, v)\n",
    "    \n",
    "print(ir.as_hlo_text())\n",
    "for i, buffer in enumerate(output.device_buffers):\n",
    "    print(i, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca2193",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<center style=\"font-size: 14pt; font-weight: bold;\">\n",
    "    That's all folks!\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
